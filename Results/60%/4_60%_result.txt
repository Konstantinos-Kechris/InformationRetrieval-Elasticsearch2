{
  "took" : 34,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : 18277,
    "max_score" : 301.8075,
    "hits" : [
      {
        "_index" : "my_index",
        "_type" : "index",
        "_id" : "6022",
        "_score" : 98.51162,
        "_source" : {
          "identifier" : "H2020ICT2015",
          "Text" : """STREAMLINE STREAMLINE will address the competitive advantage needs of European online media businesses EOMB by delivering fast reactive analytics suitable in solving a wide array of problems including addressing customer retention personalised recommendation and more broadly targeted services STREAMLINE will develop crosssectorial analytics drawing on multisource data originating from online media consumption online games telecommunications services and multilingual web content STREAMLINE partners face big and fast data challenges They serve over 100 million users offer services that produce billions of events yielding over 10 TB of data daily and possess over a PB of data at rest Their business usecases are representative of EOMB which cannot be handled efficiently  effectively by stateoftheart technologies as a consequence of system and human latencies System latency issues arise due to the lack of appropriate data streamoriented analytics tools and more importantly the added complexity cost and burden associated with jointly supporting analytics for both data at rest and data in motion Human latency results from the heterogeneity of existing tools and the low level programming languages required for development using an inordinate number of boilerplate codes that are system specific eg Hadoop SolR Esper Storm and databases and a plethora of scripts required to glue systems togetherOur research and innovation actions include addressing the challenges brought on by system and human latencies In this regard STREAMLINE will1	Develop a high level declarative language and userinterface and corresponding automatic optimisation parallelisation and system adaptation technologies that reduce the programming expertise required by data scientists thereby enabling them to more freely focus on domain specific matters2	Overcome the complexity of the socalled lambda architecture by delivering simplified operations that jointly support data at rest and data in motion in a single system and is compatible with the Hadoop ecosystem3	Develop fast reactive machine learning technologies based on distributed parameter servers and fully distributed asynchronous and approximate algorithms for fast results at high input ratesThe impact of developing a European open source tool for analysing data at rest and data in motion in a single system featuring a high level declarative language and a fast reactive machine learning library is much wider than just the recommender ad targeting and customer retention applications that the industrial partners in STREAMLINE will use to demonstrate the business value of our work for the data economy Our open source tools will help Europe in general since they lower the big data analytics skills barrier broaden the reach of data analytics tools and are applicable to diverse market sectors including healthcare manufacturing and transportation Thereby enabling a broad number of European SMEs in other markets to explore and integrate these technologies into their businesses At the same time STREAMLINE will provide a solid foundation for big data leadership in Europe by providing an opensource platform ready to be used by millions of stakeholders in companies households and governmentThe STREAMLINE consortium comprises worldrenowned scientists and innovators in the areas of database systems DFKI distributed systems SICS and machine learning SZTAKI who have won many international awards hold 18 patents collectively and have founded and advised nine startups Complementing the research excellence are four leading European enterprises in the data economy in the areas of global telecommunication services eg Internet IPTV mobile and landline networks PT games and entertainment Rovio media content streaming NMusic and webscale data extraction and business analytics IMR with Petab""",
          "Rcn" : "199862",
          "Acronym" : "STREAMLINE"
        }
      },
      {
        "_index" : "my_index",
        "_type" : "index",
        "_id" : "11990",
        "_score" : 98.18857,
        "_source" : {
          "identifier" : "H2020MSCAIF2016",
          "Text" : "Next gEneration Sequence sTORage Sequential data are everywhere from DNA sequences to astronomical light curves and from aircraft engine monitoring data to the prices of stock options Recent advances in various fields such as those of data storage networking and sensing technologies have allowed organizations to gather overwhelming amounts of sequential data at unprecedented speedsThis wealth of information enables analysts to identify patterns find abnormalities and extract knowledge It is noteworthy that common practice in various domains is to use custom data analysis solutions usually built using higher level programming languages such as RPython Such techniques however while commonly acceptable in small data processing scenarios are unfit for larger scale data management and exploration This is because they come in contrast to all previous database research not taking advantage of indexes physical data independence query optimization and data processing methods designed for scalability In these domains database systems are used merely for storing and retrieving data and not as the sophisticated query processing systems they areCurrent relational storage layers cannot handle the access patterns that analysts of sequential data are interested in without scanning large amounts of unnecessary data or without large processing overhead Thus making complex analytics inefficientIn order to exploit this new opportunity we plan to develop specialized data series storage and retrieval systems which will allow analysts  across different fields  to efficiently manipulate the sequences of interestThe proposed research project named NESTOR Next gEneration Sequence sTORage has the potential of great economic and social impact in Europe as multiple scientific and industrial fields are currently in need of the right tools in order to handle their massive collections of data series",
          "Rcn" : "208656",
          "Acronym" : "NESTOR"
        }
      },
      {
        "_index" : "my_index",
        "_type" : "index",
        "_id" : "10948",
        "_score" : 96.38904,
        "_source" : {
          "identifier" : "ERC2016STG",
          "Text" : "A Rigorous Approach to Consistency in Cloud Databases Modern Internet services store data in novel cloud databases which partition and replicate the data across a large number of machines and a wide geographical span To achieve high availability and scalability cloud databases need to maximise the parallelism of data processing Unfortunately this leads them to weaken the guarantees they provide about data consistency to applications The resulting programming models are very challenging to use correctly and we currently do not have advanced methods and tools that would help programmers in this taskThe goal of the project is to develop a synergy of novel reasoning methods static analysis tools and database implementation techniques that maximally exploit parallelism inside cloud databases while enabling application programmers to ensure correctness We intend to achieve this by first developing methods for reasoning formally about how weakening the consistency guarantees provided by cloud databases affects application correctness and the parallelism allowed inside the databases This will build on techniques from the areas of programming languages and software verification The resulting theory will then serve as a basis for practical implementation techniques and tools that harness database parallelism but only to the extent such that its side effects do not compromise application correctnessThe proposed project is highrisk because it aims not only to develop a rigorous theory of consistency in cloud databases but also to apply it to practical systems design The project is also highgain since it will push the envelope in availability scalability and costeffectiveness of cloud databases",
          "Rcn" : "207381",
          "Acronym" : "RACCOON"
        }
      },
      {
        "_index" : "my_index",
        "_type" : "index",
        "_id" : "10066",
        "_score" : 91.600655,
        "_source" : {
          "identifier" : "H2020ICT20161",
          "Text" : "European Cloud InMemory Database Appliance with Predictable Performance for Critical Applications The project aims at producing a European Cloud Database Appliance for providing a Database as a Service able to match the predictable performance robustness and trustworthiness of on premise architectures such as those based on mainframes The project will evolve cloud architectures to enable the increase of the uptake of cloud technology by providing the robustness trustworthiness and performance required for applications currently considered too critical to be deployed on existing cloudsCloudDBAppliance will deliver a cloud database appliance featuring1    A scalable operational database able to process high update workloads such as the ones processed by banks or telcos combined with a fast analytical engine able to answer analytical queries in an online manner 2    A Hadoop data lake integrated with the operational database to cover the needs from companies on big data3    A cloud hardware appliance leveraging the next generation of hardware to be produced by Bull the main European hardware provider This hardware is a scaleup hardware similar to the one of mainframes but with a more modern architecture Both the operational database and the inmemory analytics engine will be optimized to fully exploit this hardware and deliver predictable performance Additionally CloudDBAppliance will deal with the need to tolerate catastrophic cloud data centres failures eg a fire or natural disaster providing data redundancy across cloud data centres",
          "Rcn" : "206178",
          "Acronym" : "CloudDBAppliance"
        }
      },
      {
        "_index" : "my_index",
        "_type" : "index",
        "_id" : "5999",
        "_score" : 90.80941,
        "_source" : {
          "identifier" : "H2020ICT2015",
          "Text" : "Scalable online machine learning for predictive analytics and realtime interactive visualization PROTEUS mission is to investigate and develop readytouse scalable online machine learning algorithms and interactive visualization techniques for realtime predictive analytics to deal with extremely large data sets and data streams The developed algorithms and techniques will form a library to be integrated into an enhanced version of Apache Flink the EU Big Data platform PROTEUS will contribute to the EU Big Data area by addressing fundamental challenges related to the scalability and responsiveness of analytics capabilities The requirements are defined by a steelmaking industrial use case The techniques developed in PROTEUS are however general flexible and portable to all data streambased domains In particular the project will go beyond the current stateofart technology by making the following specific original contributionsi Realtime scalable machine learning for massive highvelocity and complex data streams analyticsii Realtime hybrid computation batch data and data streamsiii Realtime interactive visual analytics for Big Dataiv Enhancement of Apache Flink the EU Big Data platform andv Realworld industrial validation of the technology developedThe PROTEUS impact is manifold i strategic by reducing the gap and dependency from the US technology empowering the EU Big Data industry through the enrichment of the EU platform Apache Flink ii economic by fostering the development of new skills and new job positions and opportunities towards economic growth iii industrial by considering realworld requirements from industry and by validating the outcome on an operational setting and iv scientific by developing original hybrid and streaming analytic architectures that enable scalable online machine learning strategies and advanced interactive visualisation techniques that are applicable for general data streams in other domains",
          "Rcn" : "199839",
          "Acronym" : "PROTEUS"
        }
      },
      {
        "_index" : "my_index",
        "_type" : "index",
        "_id" : "6806",
        "_score" : 90.30758,
        "_source" : {
          "identifier" : "ERC2015CoG",
          "Text" : "Translating from Multiple Modalities into Text Recent years have witnessed the development of a wide range of computational methods that process and generate natural language text  Many of these have become familiar to mainstream computer users such as tools that retrieve documents matching a query perform sentiment analysis and translate between languages Systems like Google Translate can instantly translate between any pair of over fifty human languages allowing users to read web content that wouldnt have otherwise been available The accessibility of the web could be further enhanced with applications that translate within the same language between different modalities or different data formats  There are currently no standard tools for simplifying language eg for lowliteracy readers or second language learners The web is rife with nonlinguistic data eg databases images source code that cannot be searched since most retrieval tools operate over textual data In this project we maintain that in order to render electronic data more accessible to individuals and computers alike new types of models need to be developed Our proposal is to provide a unified framework for translating from comparable corpora ie collections consisting of data in the same or different modalities that address the same topic without being direct translations of each other We will develop general and scalable models that can solve different translation tasks and learn the necessary intermediate representations of the units involved in an unsupervised manner without extensive feature engineering Thanks to recent advances in deep  learning we will induce  representations for different modalities their interactions and correspondence to natural language Beyond addressing a fundamental aspect of the translation problem the proposed research will lead to novel internetbased applications that simplify and summarize text produce documentation for source code and meaningful descriptions for images",
          "Rcn" : "200779",
          "Acronym" : "TransModal"
        }
      },
      {
        "_index" : "my_index",
        "_type" : "index",
        "_id" : "15355",
        "_score" : 88.828766,
        "_source" : {
          "identifier" : "H2020SC6COCREATION2017",
          "Text" : "Big DATA approaches FOR improved monitoring of research and innovation performance and assessment of the societal IMPACT in the Health Demographic Change and Wellbeing Societal Challenge Recent technological developments in data mining data treatment and data analysis have been rapid and far reaching offering new dimensions and opportunities for performance analytics in various domains The introduction of new technologies and initiatives including open access mechanisms and social mediaonline media has been generating increasing volumes of new data on the research domain Data4Impact aims to capitalise on these developments and utilise big data approaches to improve the monitoring of research and innovation performance and assessment of the societal impact in the Health Demographic Change and Wellbeing Societal Challenge The project has he following main objectives a define develop analyse and disseminate new indicators for assessing the performance of EU and national research and innovation systems b explore and collect big data on healthrelated societal challenges at input throughput outputresult and impact levels c employ big data approaches to yield more data on the societal impact of national and EU funding on tackling healthrelated societal challenges d engage stakeholders in the project activities validate the project results and develop new indicators and tools using a handson approach It will do so by developing a robust conceptual model addressing all key stages of the innovation process mining large volumes of data on research results and impacts and analysis of these data using topic modelling machine learning and other techniques aimed at natural language processing The Data4Impact consortium possesses specialist knowledge of the health domain  indicator systems and is uniquely placed to mine data and apply big data approaches thanks to the partners longstanding involvement in OA einfrastructures and big data analytics Through these objectives activities and competences Data4Impact addresses both key issues described in the specific COCREATION0820162017 call",
          "Rcn" : "212386",
          "Acronym" : "Data4Impact"
        }
      },
      {
        "_index" : "my_index",
        "_type" : "index",
        "_id" : "11313",
        "_score" : 86.783806,
        "_source" : {
          "identifier" : "ERC2016COG",
          "Text" : "The Computational Database for Real World Awareness Two major hardware trends have a significant impact on the architecture of database management systems DBMSs First main memory sizes continue to grow significantly Machines with 1TB of main memory and more are readily available at a relatively low price Second the number of cores in a system continues to grow from currently 64 and more to hundreds in the near future This trend offers radically new opportunities for both business and science  It promises to allow for informationatyourfingertips ie large volumes of data can be analyzed and deeply explored online in parallel to regular transaction processing Currently deep data exploration is performed outside of the database system which necessitates huge data transfers This impedes the processing such that realtime interactive exploration is impossible  These new hardware capabilities now allow to build a true computational database system that integrates deep exploration functionality at the source of the data This will lead to a drastic shift in how users interact with data as for the first time interactive data exploration becomes possible at a massive scaleUnfortunately traditional DBMSs are simply not capable to tackle these new challengesTraditional techniques like interpreted code execution for query processing become a severe bottleneck in the presence of such massive parallelism causing poor utilization of the hardware I pursue a radically different approach Instead of adapting the traditional diskbased approaches I am integrating a new justintime compilation framework into the inmemory database that directly exploits the abundant parallel hardware for largescale data processing and exploration By explicitly utilizing cores I will be able to build a powerful computational database engine that scales the entire spectrum of data processing  from transactional to analytical to exploration workflows  far beyond traditional architectures",
          "Rcn" : "207899",
          "Acronym" : "CompDB"
        }
      },
      {
        "_index" : "my_index",
        "_type" : "index",
        "_id" : "13768",
        "_score" : 84.2429,
        "_source" : {
          "identifier" : "H2020MSCAIF2016",
          "Text" : "Biomedical Information Synthesis with Deep Natural Language Inference Deep neural networks DNNs have become a critical tool in natural language processing NLP for a wide variety of language technologies from syntax to semantics to pragmatics In particular in the field of natural language inference NLI DNNs have become the defacto model providing significantly better results than previous paradigms Their power lies in their ability to embed complex language ambiguities in high dimensional spaces coupled with nonlinear compositional transformations learned to directly optimize taskspecific objective functions We propose to adapt Deep NLI techniques to the biomedical domain specifically investigating question answering information extraction and synthesis The biomedical domain presents many key challenges and a critical impact that standard NLI challenges do not posses First while standard NLI data sets requires a system to model basic world knowledge eg that soccer is a sport they do not presume a rich domain knowledge encoded in various and often heterogeneous resources such as scientific articles textbooks and structured databases Second while standard NLI data sets presume that the answerinference is encoded in a single utterance the ability to reason and extract information from biomedical domains often requires information synthesis from multiple utterances paragraphs and even documents Finally whereas standard NLI is a broad challenge aimed at testing whether computers can make general inferences in language biomedical texts are a grounded and impactful domain where progress in automated reasoning will directly impact the efficacy of researchers physicians publishers and policy makers",
          "Rcn" : "210588",
          "Acronym" : "DNLIBiomed"
        }
      },
      {
        "_index" : "my_index",
        "_type" : "index",
        "_id" : "10696",
        "_score" : 83.860504,
        "_source" : {
          "identifier" : "H2020MSCARISE2016",
          "Text" : "Learning and Analysing Massive  Big complex Data LAMBDA aims at transferring game changing technologies to the European industry in critical areas of Machine learning Based on recent algorithmic breakthroughs we adapt sophisticated methods to targeted industries with a twofold goal First to help turn cutting edge tools into innovative software products and processes tailored to realworld issues Second based on the available data to organise open data repositories and benchmarks or to simulate data with the same statistical properties when such data is confidential following anonymisationLAMBDA focuses on two distinct application domains 3D shape analysis and unstructured data mining They share challenging features such as inherent complexity in modeling the data high dimensionality which raises the issue of curse of dimensionality and the need to address such datasets at a massive scale Moreover they correspond to the expertise of the participants LAMBDA is characterised by a unique blend of theoretically rigorous and geometrically inclined methods thus supporting a strong aspect of interdisciplinarity between Theory of Algorithms and Machine Learning This shall be supported by advanced software development ranging from publicdomain prototype implementations to licensed software and integrated libraries where the latter is based on the highlyoptimised platform BIDMach developed at UC BerkeleyLAMBDA strengthens existing links within Europe and across the Atlantic while creating new synergies in the directions of two industrial domains namely 3D shape search and insurance data The two clusters are organised around representative companies in the respective domains The Project is built so as to support knowledge transfer beyond its lifetime Besides intersectoral collaborations we exploit the international dimension by associating leading USA Universities so as to bring stateoftheart methods developed at the global level into the European framework",
          "Rcn" : "207038",
          "Acronym" : "LAMBDA"
        }
      },
      {
        "_index" : "my_index",
        "_type" : "index",
        "_id" : "1159",
        "_score" : 83.541374,
        "_source" : {
          "identifier" : "H2020ICT20141",
          "Text" : "MMT will deliver a language independent commercial online translation service based on a new opensource machine translation distributed architecture The goal of MMT is to deliver a language independent commercial online translation service based on a new opensource machine translation distributed architecture MMT does not require any initial training phase Once fed with training data MMT will be ready to translate MMT defacto will merge translation memory and machine translation technology into one single product Quality of translations will increase as soon as new training data are addedMMT manages context automatically so that it will not require building domain specific systems MMT will provide best translation quality for any topicdomain by storing training segments together with context linking informationMMT enables scalability of data and users so that no more expensive adhoc hardware installations are needed The MMT architecture will support high performance and linear scalability up to thousands of nodes The same software will work to setup a personal translation system or to create a webbased service on a cluster of commodity nodes able to handle terabytes of data and millions of usersMMT will create a data collection infrastructure that accelerates the process of filling the data gap between large IT companies and the MT industry  MMT will leverage the data crawled on the web by Common Crawl TAUS Translateds MyMemory and Matecat data and facilities to set up a processing pipeline that will create unprecedented  amounts of clean parallel and monolingual data to develop machine translation systems",
          "Rcn" : "194327",
          "Acronym" : "MMT"
        }
      },
      {
        "_index" : "my_index",
        "_type" : "index",
        "_id" : "15970",
        "_score" : 80.07383,
        "_source" : {
          "identifier" : "H2020ICT20171",
          "Text" : "Edge and CLoud Computation A Highly Distributed Software Architecture for Big Data AnalyticS Big data applications processing extreme amounts of complex data are nowadays being integrated with even more challenging requirements such as the need of continuously processing vast amount of information in realtimeCurrent data analytics systems are usually designed following two conflicting  priorities to provide i a quick and reactive response referred to as datainmotion analysis possibly in realtime based on continuous data flows or ii a thorough and more computationally intensive feedback referred to as dataatrest analysis which typically implies aggregating more information into larger models Given the apparently incompatible requirements these approaches have been tackled separately although they provide complementary capabilitiesCLASS aims to develop a novel software architecture to help big data developers to combine datainmotion and dataatrest analysis by efficiently distributing data and process mining along the compute continuum from edge to cloud in a complete and transparent way while providing sound realtime guarantees CLASS aims at adopting 1 innovative distributed architectures from the highperformance domain 2 timing analysis methods and energyefficient parallel architectures from the embedded domain and 3 data analytics platforms and programming models from the bigdata domainThe capabilities of the CLASS framework will be demonstrated on a real smartcity use case featuring a heavy sensor infrastructure to collect realtime data across a wide urban area and prototype cars equipped with heterogeneous sensorsactuators V2I connectivity and cluster support to present the innovative capabilities to drivers Representative applications for traffic management and advanced driving assistance domains have been selected to efficiently process very large heterogeneous data streams in realtime providing innovative services while preparing the technological background for the advent of autonomous vehicles",
          "Rcn" : "213159",
          "Acronym" : "CLASS"
        }
      },
      {
        "_index" : "my_index",
        "_type" : "index",
        "_id" : "5200",
        "_score" : 79.130325,
        "_source" : {
          "identifier" : "H2020MSCARISE2015",
          "Text" : "EnhaNcing seCurity And privacy in the Social wEb a user centered approach for the protection of minors ENCASE will leverage the latest advances in usable security and privacy to design and implement a browserbased architecture for the protection of minors from malicious actors in online social networks The ENCASE usercentric architecture will consist of three distinct services which can be combined to form an effective protective net against cyberbullying and sexually abusive acts a a browser addon with its corresponding scalable backend software stack that collects the users online actions to unveil incidents of aggressive or  distressed behavior b a browser addon  with its associated scalable software stack that analyses social web data to detect fraudulent and fake activity and alert the user and c a browser addon that detects when a user is about to share sensitive content eg photos or address information with an inappropriate audience and warns the user or his parents of the imminent privacy threat The third addon has usable controls that enable users to protect their content by suggesting suitable access lists by watermarking and by securing the content via cryptography or steganography The three browser addons and the backend social web data analytics software stack will be assessed with user studies and piloting activities and will be released to the public The foundation of the research and innovation activities will be a diligently planned intersectorial and interdisciplinary secondment program for Experienced and Early Stage Researchers that fosters knowledge exchange  The academic partners will contribute knowhow on user experience assessment large scale data processing machine learning and datamining algorithm design and content confidentiality techniques The industrial partners will primarily offer expertise in productiongrade software development access to realworld online social network data and access to numerous endusers through widely deployed products",
          "Rcn" : "198839",
          "Acronym" : "ENCASE"
        }
      },
      {
        "_index" : "my_index",
        "_type" : "index",
        "_id" : "1017",
        "_score" : 78.96763,
        "_source" : {
          "identifier" : "H2020ICT20141",
          "Text" : "Scalable Hybrid Variability for Distributed Evolving Software Systems HyVar proposes a development framework for continuous and individualized evolution of distributed software applications running on remote devices in heterogeneous environments The framework will combine variability modeling from software product lines with formal methods and software upgrades and be integrated in existing software development processes HyVars objectives are O1 to develop a Domain Specific Variability Language DSVL and tool chain to support software variability for such applications O2 to develop a cloud infrastructure that exploits software variability as described in the DSVL to track the software configurations deployed on remote devices and to enable i the collection of data from the devices to monitor their behavior and ii secure and efficient customized updates O3 to develop a technology for overtheair updates of distributed applications which enables continuous software evolution after deployment on complex remote devices that incorporate a system of systems and O4 to test HyVars approach as described in the above objectives in an industryled demonstrator to assess in quantifiable ways its benefits HyVar goes beyond the stateoftheart by proposing hybrid variability  ie the automatic generation and deployment of software updates combines the variability model  describing  possible software configurations  with sensor data collected from the device HyVars scalable cloud infrastructure will elastically support monitoring and customization for numerous application instances Software analysis will exploit the structure of the variability models Upgrades will be seamless and sufficiently nonintrusive to enhance the user quality experience without compromising the robustness reliability and resilience of the distributed application instances To maximize impact and innovation the consortium balances carefully selected academic and industrial partners ensuring both technology pull and push",
          "Rcn" : "194185",
          "Acronym" : "HyVar"
        }
      },
      {
        "_index" : "my_index",
        "_type" : "index",
        "_id" : "18245",
        "_score" : 78.53963,
        "_source" : {
          "identifier" : "H2020FETHPC2017",
          "Text" : "ASPIDE exAScale ProgramIng models for extreme Data procEssing Extreme Data is an incarnation of Big Data concept distinguished by the massive amounts of data that must be queried communicated and analyzed in near realtime by using a very large number of memorystorage elements and Exascale computing systems Immediate examples are the scientific data produced at a rate of hundreds of gigabitspersecond that must be stored filtered and analyzed the millions of images per day that must be mined analyzed in parallel the one billion of social data posts queried in realtime on an inmemory  components database Traditional disks or commercial storage cannot handle nowadays the extreme scale of such application data Following the need of improvement of current concepts and technologies ASPIDEs activities focus on dataintensive applications running on systems composed of up to millions of computing elements Exascale systems Practical results will include the methodology and software prototypes that will be designed and used to implement Exascale applicationsThe ASPIDE project will contribute with the definition of a new programming paradigms APIs runtime tools and methodologies for expressing dataintensive tasks on Exascale systems which can  pave the way for the exploitation of massive parallelism over a simplified model of the system architecture promoting high performance and efficiency and offering powerful operations and mechanisms for processing extreme data sources at high speed andor realtime",
          "Rcn" : "215834",
          "Acronym" : "ASPIDE"
        }
      },
      {
        "_index" : "my_index",
        "_type" : "index",
        "_id" : "1093",
        "_score" : 78.486984,
        "_source" : {
          "identifier" : "H2020ICT20141",
          "Text" : "AEGLE Ancient Greek    An analytics framework for integrated and personalized healthcare services in Europe The data generated in the health domain is coming from heterogeneous multimodal multilingual dynamic and fast evolving medical technologies Today we are found in a big health landscape characterized by large volume versatility and velocity 3Vs which has led to the evolution of the informatics in the big biodata domain AEGLE project will build an innovative ICT solution addressing the whole data value chain for health based on cloud computing enabling dynamic resource allocation HPC infrastructures for computational acceleration and advanced visualization techniques AEGLE will Realize a multiparametric platform using algorithms for analysing big biodata including features such as volume properties communication metrics and bottlenecks estimation of related computational resources needed handling data versatility and managing velocity  Address the systemic health big biodata in terms of the 3V multidimensional space using analytics based on PCA techniques  Demonstrate AEGLEs efficiency through the provision of aggregated services covering the 3V space of big biodata Specifically it will be evaluated in abig biostreams where the decision speed is critical and needs nonlinear and multiparametric estimators for clinical decision support within limited time bbigdata from nonmalignant diseases where the need for NGS and molecular data analytics requires the combination of cloud located resources coupled with local demands for data and visualization and finally cbigdata from chronic diseases including EHRs and medication with needs for quantified estimates of important clinical parameters semantics extraction and regulatory issues for integrated care  Bring together all related stakeholders leading to integration with existing open databases increasing the speed of AEGLE adaptation  Build a business ecosystem for the wider exploitation and targeting on crossborder production of custom multilingual solutions based on AEGLE",
          "Rcn" : "194261",
          "Acronym" : "AEGLE"
        }
      },
      {
        "_index" : "my_index",
        "_type" : "index",
        "_id" : "5754",
        "_score" : 78.45892,
        "_source" : {
          "identifier" : "H2020ICT2015",
          "Text" : "Holistic Benchmarking of Big Linked Data Linked Data has gained significant momentum over the last years It is now used at industrial scale in many sectors in which an increasingly large amount of rapidly changing data needs to be processed HOBBIT is an ambitious project that aims to push the development of Big Linked Data BLD processing solutions by providing a family of industryrelevant benchmarks for the BLD value chain through a generic evaluation platform We aim to make open deterministic benchmarks available to test the performance of existing systems and push the development of innovative industryrelevant solutions The underlying data will mimic real industrial data assembled during the course of the project At the beginning of the project HOBBIT will work on roughly 1PB of real industryrelevant data from 4 different domains The data will be extended through collaborations during the project To push the use of the benchmarks we will organize or join challenges that aim to measure the performance of technologies for the different steps of the BLD lifecycle In contrast to existing benchmarks we will provide modular and easily extensible benchmarks for all industryrelevant BLD processing steps that allow to assess whole suites of software that cover more than one stepThe infrastructure necessary to run the evaluation campaigns will be made available Our architecture will rely on web interfaces and cloud infrastructures to ensure scalability The open HOBBIT platform will make human and machinereadable public periodic reports available As exit strategy the project will create an association after the second project year that will be sustained by the means of subscriptions from industry and academia and associated with existing benchmarking associations The clear portfolio of added value for the members will be defined in the early project stages and disseminated throughout the evaluation campaigns",
          "Rcn" : "199489",
          "Acronym" : "HOBBIT"
        }
      },
      {
        "_index" : "my_index",
        "_type" : "index",
        "_id" : "12430",
        "_score" : 78.06046,
        "_source" : {
          "identifier" : "ERC2016COG",
          "Text" : "Robust algorithms for learning from modern data Machine learning is needed and used everywhere from science to industry with a growing impact on many disciplines While first successes were due at least in part to simple supervised learning algorithms used primarily as black boxes on mediumscale problems modern data pose new challenges Scalability is an important issue of course with large amounts of data many current problems far exceed the capabilities of existing algorithms despite sophisticated computing architectures But beyond this the core classical model of supervised machine learning with the usual assumptions of independent and identically distributed data or welldefined features outputs and loss functions has reached its theoretical and practical limitsGiven this new setting existing optimizationbased algorithms are not adapted The main objective of this proposal is to push the frontiers of supervised machine learning in terms of a scalability to data with massive numbers of observations features and tasks b adaptability to modern computing environments in particular for parallel and distributed processing c provable adaptivity and robustness to problem and hardware specifications and d robustness to nonconvexities inherent in machine learning problemsTo achieve the expected breakthroughs we will design a novel generation of learning algorithms amenable to a tight convergence analysis with realistic assumptions and efficient implementations They will help transition machine learning algorithms towards the same widespread robust use as numerical linear algebra libraries Outcomes of the research described in this proposal will include algorithms that come with strong convergence guarantees and are welltested on reallife benchmarks coming from computer vision bioinformatics audio processing and natural language processing For both distributed and nondistributed settings we will release opensource software adapted to widely available computing platforms",
          "Rcn" : "209141",
          "Acronym" : "SEQUOIA"
        }
      },
      {
        "_index" : "my_index",
        "_type" : "index",
        "_id" : "10070",
        "_score" : 77.51682,
        "_source" : {
          "identifier" : "H2020ICT20161",
          "Text" : "DeveloperCentric Knowledge Mining from Large OpenSource Software Repositories Recent reports state that the adoption of opensource software OSS helps resulting in savings of about 60 billion per year to consumers However the use of OSS also comes at enormous cost choosing among OSS projects and maintaining dependence on continuously changing software requires a large investment Deciding if an OSS project meets the required standards for adoption is hard and keeping uptodate with an evolving project is even harder It involves analysing code documentation online discussions and issue trackers There is too much information to process manually and it is common that uninformed decisions have to be made with detrimental effectsCROSSMINER remedies this by automatically extracting the required knowledge and injecting it into the IDE of the developers at the time they need it to make their design decisions This allows them to reduce their effort in knowledge acquisition and to increase the quality of their code CROSSMINER uniquely combines advanced software project analyses with online monitoring in the IDE The developer will be monitored to infer which information is timely based on readily available knowledge stored earlier by a set of advanced offline deep analyses of related OSS projects To achieve this timely and ambitious goal CROSSMINER combines six enduser partners in the domains of IoT multisector IT services API coevolution software analytics software quality assurance and OSS forges along with RD partners that have a long trackrecord in conducting cuttingedge research on largescale software analytics natural language processing reverse engineering of software components modeldriven engineering and delivering results in the form of widelyused sustainable and industrialstrength OSS The development of the CROSSMINER platform is guided by an advisory board of worldclass experts and the dissemination of the project will be led by The Open Group",
          "Rcn" : "206182",
          "Acronym" : "CROSSMINER"
        }
      },
      {
        "_index" : "my_index",
        "_type" : "index",
        "_id" : "6292",
        "_score" : 77.414604,
        "_source" : {
          "identifier" : "ERC2015CoG",
          "Text" : "Foundations of Factorized Data Management Systems The objective of this project is to investigate scalability questions arising with a new wave of smart relational data management systems that integrate analytics and query processing These questions will be addressed by a fundamental shift from centralized processing on tabular data representation as supported by traditional systems and analytics software packages to distributed and approximate processing on factorized data representationFactorized representations exploit algebraic properties of relational algebra and the structure of queries and analytics to achieve radically better data compression than generic compression schemes while at the same time allowing processing in the compressed domain They can effectively boost the performance of relational processing by avoiding redundant computation in the oneserver setting yet they can also be naturally exploited for approximate and distributed processing Large relations can be approximated by their subsets and supersets ie lower and upper bounds that factorize much better than the relations themselves Factorizing relations which represent intermediate results shuffled between servers in distributed processing can effectively reduce the communication cost and improve the latency of the systemThe key deliverables will be novel algorithms that combine distribution approximation and factorization for computing mixed loads of queries and predictive and descriptive analytics on largescale data This research will result in fundamental theoretical contributions such as complexity results for largescale processing and tractable algorithms and also in a scalable factorized data management system that will exploit these theoretical insights We will collaborate with industrial partners who are committed to assist in providing datasets and realistic workloads infrastructure for largescale distributed systems and support for transferring the products of the research to industrial users",
          "Rcn" : "200139",
          "Acronym" : "FADAMS"
        }
      }
    ]
  }
}
