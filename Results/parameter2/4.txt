{
  "took" : 49,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : 309,
    "max_score" : 73.69672,
    "hits" : [
      {
        "_index" : "my_index",
        "_type" : "index",
        "_id" : "5999",
        "_score" : 27.88275,
        "_source" : {
          "identifier" : "H2020ICT2015",
          "Text" : "Scalable online machine learning for predictive analytics and realtime interactive visualization PROTEUS mission is to investigate and develop readytouse scalable online machine learning algorithms and interactive visualization techniques for realtime predictive analytics to deal with extremely large data sets and data streams The developed algorithms and techniques will form a library to be integrated into an enhanced version of Apache Flink the EU Big Data platform PROTEUS will contribute to the EU Big Data area by addressing fundamental challenges related to the scalability and responsiveness of analytics capabilities The requirements are defined by a steelmaking industrial use case The techniques developed in PROTEUS are however general flexible and portable to all data streambased domains In particular the project will go beyond the current stateofart technology by making the following specific original contributionsi Realtime scalable machine learning for massive highvelocity and complex data streams analyticsii Realtime hybrid computation batch data and data streamsiii Realtime interactive visual analytics for Big Dataiv Enhancement of Apache Flink the EU Big Data platform andv Realworld industrial validation of the technology developedThe PROTEUS impact is manifold i strategic by reducing the gap and dependency from the US technology empowering the EU Big Data industry through the enrichment of the EU platform Apache Flink ii economic by fostering the development of new skills and new job positions and opportunities towards economic growth iii industrial by considering realworld requirements from industry and by validating the outcome on an operational setting and iv scientific by developing original hybrid and streaming analytic architectures that enable scalable online machine learning strategies and advanced interactive visualisation techniques that are applicable for general data streams in other domains",
          "Rcn" : "199839",
          "Acronym" : "PROTEUS"
        }
      },
      {
        "_index" : "my_index",
        "_type" : "index",
        "_id" : "11985",
        "_score" : 25.399757,
        "_source" : {
          "identifier" : "H2020MSCAIF2016",
          "Text" : "Multilanguage text summarization In our daily life we are submerged by huge amounts of text coming from different sources such as emails news reports and so on The availability of unprecedented volumes of data represents both a challenge and an opportunity On one hand it can lead to information overload a phenomenon that limits ones capacity to understand an issue and act in the presence of too much information On the other hand the effective harnessing of this information has undeniable economical potential Furthermore In the European context special needs to be put to multilingualism to guarantee global access to high quality informationThe objective of this application is to develop MLTEXTSUM a system for efficient and accurate multilingual text summarization That is given as input a text document the system will output a summary of the document in the same or in a different language Building on recent breakouts in machine learning and natural language processing I propose a novel architecture for MLTEXTSUM that will be able to produce high quality summaries while at same time remain modular enough so that new languages can be added with minimal effort The availability of such system shall allow citizens regardless of their language to better handle the information overload and to gain access to critically distilled information eg what is a certain newspapers opinion on the same topic this year Are malefemale athletes portrayed differently by the media The project is characterized by the interplay of multiple disciplines the proposed architecture requires to master a combination of natural language processing and machine learning techniques At the same time the formidable scale of this system will require the development of novel distributed optimization methods This interplay will be achieved thanks to my past and future collaborations my solid background in optimization and machine learning as well as through the acquisition of new adhoc skills",
          "Rcn" : "208651",
          "Acronym" : "ML-TEXTSUM"
        }
      },
      {
        "_index" : "my_index",
        "_type" : "index",
        "_id" : "6806",
        "_score" : 24.383778,
        "_source" : {
          "identifier" : "ERC2015CoG",
          "Text" : "Translating from Multiple Modalities into Text Recent years have witnessed the development of a wide range of computational methods that process and generate natural language text  Many of these have become familiar to mainstream computer users such as tools that retrieve documents matching a query perform sentiment analysis and translate between languages Systems like Google Translate can instantly translate between any pair of over fifty human languages allowing users to read web content that wouldnt have otherwise been available The accessibility of the web could be further enhanced with applications that translate within the same language between different modalities or different data formats  There are currently no standard tools for simplifying language eg for lowliteracy readers or second language learners The web is rife with nonlinguistic data eg databases images source code that cannot be searched since most retrieval tools operate over textual data In this project we maintain that in order to render electronic data more accessible to individuals and computers alike new types of models need to be developed Our proposal is to provide a unified framework for translating from comparable corpora ie collections consisting of data in the same or different modalities that address the same topic without being direct translations of each other We will develop general and scalable models that can solve different translation tasks and learn the necessary intermediate representations of the units involved in an unsupervised manner without extensive feature engineering Thanks to recent advances in deep  learning we will induce  representations for different modalities their interactions and correspondence to natural language Beyond addressing a fundamental aspect of the translation problem the proposed research will lead to novel internetbased applications that simplify and summarize text produce documentation for source code and meaningful descriptions for images",
          "Rcn" : "200779",
          "Acronym" : "TransModal"
        }
      },
      {
        "_index" : "my_index",
        "_type" : "index",
        "_id" : "5735",
        "_score" : 23.951866,
        "_source" : {
          "identifier" : "ERC2015STG",
          "Text" : "Induction of BroadCoverage Semantic Parsers In the last one or two decades language technology has achieved a number of important successes for example producing functional machine translation systems and beating humans in quiz games The key bottleneck which prevents further progress in these and many other natural language processing NLP applications eg text summarization information retrieval opinion mining dialog and tutoring systems is the lack of accurate methods for producing meaning representations of texts Accurately predicting such meaning representations on an open domain with an automatic parser is a challenging and unsolved problem primarily because of language variability and ambiguity The reason for the unsatisfactory performance is reliance on supervised learning learning from annotated resources with the amounts of annotation required for accurate opendomain parsing exceeding what is practically feasible  Moreover representations defined in these resources typically do not provide abstractions suitable for reasoning In this project we will induce semantic representations from large amounts of unannotated data ie text which has not been labeled by humans while guided by information contained in humanannotated data and other forms of linguistic knowledge This will allow us to scale our approach to many domains and across languages We will specialize meaning representations for reasoning by modeling relations eg facts appearing across sentences in texts documentlevel modeling across different texts and across texts and knowledge bases Learning to predict this linked data is closely related to learning to reason including learning the notions of semantic equivalence and entailment We will jointly induce semantic parsers eg loglinear featurerich models and reasoning models latent factor models relying on this data thus ensuring that the semantic representations are informative for applications requiring reasoning",
          "Rcn" : "199469",
          "Acronym" : "BroadSem"
        }
      },
      {
        "_index" : "my_index",
        "_type" : "index",
        "_id" : "11990",
        "_score" : 23.786367,
        "_source" : {
          "identifier" : "H2020MSCAIF2016",
          "Text" : "Next gEneration Sequence sTORage Sequential data are everywhere from DNA sequences to astronomical light curves and from aircraft engine monitoring data to the prices of stock options Recent advances in various fields such as those of data storage networking and sensing technologies have allowed organizations to gather overwhelming amounts of sequential data at unprecedented speedsThis wealth of information enables analysts to identify patterns find abnormalities and extract knowledge It is noteworthy that common practice in various domains is to use custom data analysis solutions usually built using higher level programming languages such as RPython Such techniques however while commonly acceptable in small data processing scenarios are unfit for larger scale data management and exploration This is because they come in contrast to all previous database research not taking advantage of indexes physical data independence query optimization and data processing methods designed for scalability In these domains database systems are used merely for storing and retrieving data and not as the sophisticated query processing systems they areCurrent relational storage layers cannot handle the access patterns that analysts of sequential data are interested in without scanning large amounts of unnecessary data or without large processing overhead Thus making complex analytics inefficientIn order to exploit this new opportunity we plan to develop specialized data series storage and retrieval systems which will allow analysts  across different fields  to efficiently manipulate the sequences of interestThe proposed research project named NESTOR Next gEneration Sequence sTORage has the potential of great economic and social impact in Europe as multiple scientific and industrial fields are currently in need of the right tools in order to handle their massive collections of data series",
          "Rcn" : "208656",
          "Acronym" : "NESTOR"
        }
      },
      {
        "_index" : "my_index",
        "_type" : "index",
        "_id" : "10066",
        "_score" : 23.317554,
        "_source" : {
          "identifier" : "H2020ICT20161",
          "Text" : "European Cloud InMemory Database Appliance with Predictable Performance for Critical Applications The project aims at producing a European Cloud Database Appliance for providing a Database as a Service able to match the predictable performance robustness and trustworthiness of on premise architectures such as those based on mainframes The project will evolve cloud architectures to enable the increase of the uptake of cloud technology by providing the robustness trustworthiness and performance required for applications currently considered too critical to be deployed on existing cloudsCloudDBAppliance will deliver a cloud database appliance featuring1    A scalable operational database able to process high update workloads such as the ones processed by banks or telcos combined with a fast analytical engine able to answer analytical queries in an online manner 2    A Hadoop data lake integrated with the operational database to cover the needs from companies on big data3    A cloud hardware appliance leveraging the next generation of hardware to be produced by Bull the main European hardware provider This hardware is a scaleup hardware similar to the one of mainframes but with a more modern architecture Both the operational database and the inmemory analytics engine will be optimized to fully exploit this hardware and deliver predictable performance Additionally CloudDBAppliance will deal with the need to tolerate catastrophic cloud data centres failures eg a fire or natural disaster providing data redundancy across cloud data centres",
          "Rcn" : "206178",
          "Acronym" : "CloudDBAppliance"
        }
      },
      {
        "_index" : "my_index",
        "_type" : "index",
        "_id" : "2893",
        "_score" : 22.984009,
        "_source" : {
          "identifier" : "H2020MSCAIF2014",
          "Text" : "A Scalable and Elastic Platform for NearRealtime Analytics for The Graph of Everything The SMARTER A Scalable and Elastic Platform for NearRealtime Analytics for The Graph of EveryThing project aims to build a platform that provide the ability to derive actionable information from enormous amount of data generated by the Internet of Everything  to leverage datadriven strategies to innovate compete and capture value from deep web and realtime information The project targets innovative research outcomes by addressing Big Dynamic Data Analytic requirements from three relevant aspects variety and velocity and volume The project introduces the concept Graph of Everything GoT  to deal with the issue of data variety in data analytics for Internet of Things IoT data The Graph of Everything extends Linked Data model RDF  that has been widely used for representing deep web data to connect dynamic data from data streams generated from IoT eg sensor readings with any knowledgebase to create a single graph as an integrated database serving any analytical queries on a set of nodesedges of the graph so called analytical lens of everything The dynamic data represented as Linked Data Model called Linked Stream Data may contain valuable but perishable insights which are only valuable if it can be detected to act on them right at the right time Moreover to derive such insights the dynamic data needs to be correlated with various large datasets Therefore SMARTER has to deal both the velocity  requirements together volume requirements of analysing GoT to make the platform able support nearrealtime analytical operations with the elastically scalability",
          "Rcn" : "196064",
          "Acronym" : "SMARTER"
        }
      },
      {
        "_index" : "my_index",
        "_type" : "index",
        "_id" : "11313",
        "_score" : 22.352348,
        "_source" : {
          "identifier" : "ERC2016COG",
          "Text" : "The Computational Database for Real World Awareness Two major hardware trends have a significant impact on the architecture of database management systems DBMSs First main memory sizes continue to grow significantly Machines with 1TB of main memory and more are readily available at a relatively low price Second the number of cores in a system continues to grow from currently 64 and more to hundreds in the near future This trend offers radically new opportunities for both business and science  It promises to allow for informationatyourfingertips ie large volumes of data can be analyzed and deeply explored online in parallel to regular transaction processing Currently deep data exploration is performed outside of the database system which necessitates huge data transfers This impedes the processing such that realtime interactive exploration is impossible  These new hardware capabilities now allow to build a true computational database system that integrates deep exploration functionality at the source of the data This will lead to a drastic shift in how users interact with data as for the first time interactive data exploration becomes possible at a massive scaleUnfortunately traditional DBMSs are simply not capable to tackle these new challengesTraditional techniques like interpreted code execution for query processing become a severe bottleneck in the presence of such massive parallelism causing poor utilization of the hardware I pursue a radically different approach Instead of adapting the traditional diskbased approaches I am integrating a new justintime compilation framework into the inmemory database that directly exploits the abundant parallel hardware for largescale data processing and exploration By explicitly utilizing cores I will be able to build a powerful computational database engine that scales the entire spectrum of data processing  from transactional to analytical to exploration workflows  far beyond traditional architectures",
          "Rcn" : "207899",
          "Acronym" : "CompDB"
        }
      },
      {
        "_index" : "my_index",
        "_type" : "index",
        "_id" : "1093",
        "_score" : 22.173893,
        "_source" : {
          "identifier" : "H2020ICT20141",
          "Text" : "AEGLE Ancient Greek    An analytics framework for integrated and personalized healthcare services in Europe The data generated in the health domain is coming from heterogeneous multimodal multilingual dynamic and fast evolving medical technologies Today we are found in a big health landscape characterized by large volume versatility and velocity 3Vs which has led to the evolution of the informatics in the big biodata domain AEGLE project will build an innovative ICT solution addressing the whole data value chain for health based on cloud computing enabling dynamic resource allocation HPC infrastructures for computational acceleration and advanced visualization techniques AEGLE will Realize a multiparametric platform using algorithms for analysing big biodata including features such as volume properties communication metrics and bottlenecks estimation of related computational resources needed handling data versatility and managing velocity  Address the systemic health big biodata in terms of the 3V multidimensional space using analytics based on PCA techniques  Demonstrate AEGLEs efficiency through the provision of aggregated services covering the 3V space of big biodata Specifically it will be evaluated in abig biostreams where the decision speed is critical and needs nonlinear and multiparametric estimators for clinical decision support within limited time bbigdata from nonmalignant diseases where the need for NGS and molecular data analytics requires the combination of cloud located resources coupled with local demands for data and visualization and finally cbigdata from chronic diseases including EHRs and medication with needs for quantified estimates of important clinical parameters semantics extraction and regulatory issues for integrated care  Bring together all related stakeholders leading to integration with existing open databases increasing the speed of AEGLE adaptation  Build a business ecosystem for the wider exploitation and targeting on crossborder production of custom multilingual solutions based on AEGLE",
          "Rcn" : "194261",
          "Acronym" : "AEGLE"
        }
      },
      {
        "_index" : "my_index",
        "_type" : "index",
        "_id" : "13768",
        "_score" : 22.078487,
        "_source" : {
          "identifier" : "H2020MSCAIF2016",
          "Text" : "Biomedical Information Synthesis with Deep Natural Language Inference Deep neural networks DNNs have become a critical tool in natural language processing NLP for a wide variety of language technologies from syntax to semantics to pragmatics In particular in the field of natural language inference NLI DNNs have become the defacto model providing significantly better results than previous paradigms Their power lies in their ability to embed complex language ambiguities in high dimensional spaces coupled with nonlinear compositional transformations learned to directly optimize taskspecific objective functions We propose to adapt Deep NLI techniques to the biomedical domain specifically investigating question answering information extraction and synthesis The biomedical domain presents many key challenges and a critical impact that standard NLI challenges do not posses First while standard NLI data sets requires a system to model basic world knowledge eg that soccer is a sport they do not presume a rich domain knowledge encoded in various and often heterogeneous resources such as scientific articles textbooks and structured databases Second while standard NLI data sets presume that the answerinference is encoded in a single utterance the ability to reason and extract information from biomedical domains often requires information synthesis from multiple utterances paragraphs and even documents Finally whereas standard NLI is a broad challenge aimed at testing whether computers can make general inferences in language biomedical texts are a grounded and impactful domain where progress in automated reasoning will directly impact the efficacy of researchers physicians publishers and policy makers",
          "Rcn" : "210588",
          "Acronym" : "DNLIBiomed"
        }
      },
      {
        "_index" : "my_index",
        "_type" : "index",
        "_id" : "10948",
        "_score" : 21.681446,
        "_source" : {
          "identifier" : "ERC2016STG",
          "Text" : "A Rigorous Approach to Consistency in Cloud Databases Modern Internet services store data in novel cloud databases which partition and replicate the data across a large number of machines and a wide geographical span To achieve high availability and scalability cloud databases need to maximise the parallelism of data processing Unfortunately this leads them to weaken the guarantees they provide about data consistency to applications The resulting programming models are very challenging to use correctly and we currently do not have advanced methods and tools that would help programmers in this taskThe goal of the project is to develop a synergy of novel reasoning methods static analysis tools and database implementation techniques that maximally exploit parallelism inside cloud databases while enabling application programmers to ensure correctness We intend to achieve this by first developing methods for reasoning formally about how weakening the consistency guarantees provided by cloud databases affects application correctness and the parallelism allowed inside the databases This will build on techniques from the areas of programming languages and software verification The resulting theory will then serve as a basis for practical implementation techniques and tools that harness database parallelism but only to the extent such that its side effects do not compromise application correctnessThe proposed project is highrisk because it aims not only to develop a rigorous theory of consistency in cloud databases but also to apply it to practical systems design The project is also highgain since it will push the envelope in availability scalability and costeffectiveness of cloud databases",
          "Rcn" : "207381",
          "Acronym" : "RACCOON"
        }
      },
      {
        "_index" : "my_index",
        "_type" : "index",
        "_id" : "1058",
        "_score" : 21.646044,
        "_source" : {
          "identifier" : "H2020ICT20141",
          "Text" : "Social Semantic Emotion Analysis for Innovative Multilingual Big Data Analytics Markets MixedEmotions will develop innovative multilingual multimodal Big Data analytics applications that will analyze a more complete emotional profile of user behavior using data from mixed input channels multilingual text data sources AV signal input multilingual speech audio video social media social network comments and structured data Commercial applications implemented as pilot projects will be in Social TV Brand Reputation Management and Call Centre Operations Making sense of accumulated user interaction from different data sources modalities and languages is challenging and has not yet been explored in fullness in an industrial context Commercial solutions exist but do not address the multilingual aspect in a robust and largescale setting and do not scale up to huge data volumes that need to be processed or the integration of emotion analysis observations across data sources andor modalities on a meaningful level MixedEmotions will implement an integrated Big Linked Data platform for emotion analysis across heterogeneous data sources different languages and modalities building on existing state of the art tools services and approaches that will enable the tracking of emotional aspects of user interaction and feedback on an entity level The MixedEmotions platform will provide an integrated solution for largescale emotion analysis and fusion on heterogeneous multilingual text speech video and social media data streams leveraging open access and proprietary data sources and exploiting social context by leveraging social network graphs semanticlevel emotion information aggregation and integration through robust extraction of social semantic knowledge graphs for emotion analysis along multidimensional clusters",
          "Rcn" : "194226",
          "Acronym" : "MixedEmotions"
        }
      },
      {
        "_index" : "my_index",
        "_type" : "index",
        "_id" : "1125",
        "_score" : 21.379763,
        "_source" : {
          "identifier" : "H2020ICT20141",
          "Text" : "eNd to End scalable and dynamically reconfigurable oPtical arcHitecture for applicationawarE SDN cLoud datacentErs Datacentre traffic is experiencing 2digit growth challenging the scalability of current network architectures The new concept of disaggregation exacerbates bandwidth and latency demands whereas emerging cloud business opportunities urge for reliable interdatacenter networking PROJECT will develop an endtoend solution extending from the datacenter architecture and optical subsystem design to the overlaying control plane and application interfaces PROJECT hybrid electronicoptical network architecture scales linearly with the number of datacenter hosts offers Ethernet granularity and saves up to 94 power and 30 cost  It consolidates compute and storage networks over a single Ethernet optical TDMA network Low latency hardwarelevel dynamic reconfigurability and quasideterministic QoS are supported in view of disaggregated datacenter deployment scenarios A fully functional control plane overlay will be developed comprising an SDN controller along with its interfaces The southbound interface abstracts physical layer infrastructure and allows dynamic hardwarelevel network reconfigurability The northbound interface links the SDN controller with the application requirements through an Application Programming Interface PROJECT innovative control plane enables Application Defined Networking and merges hardware and software virtualization over the hybrid optical infrastructure It also integrates SDN modules and functions for interdatacenter connectivity enabling dynamic bandwidth allocation based on the needs of migrating VMs as well as on existing Service Level Agreements for transparent networking among telecom and datacenter operators domains Fullyfunctional network subsystems will be prototyped a 400Gbs hybrid TopofRack switch a 50Gbs electronicoptical smart Network Interface Card and a fast optical pod switch PROJECT concept will be demonstrated in the lab and in its operational environment for both intra and interdatacenter scenario",
          "Rcn" : "194293",
          "Acronym" : "NEPHELE"
        }
      },
      {
        "_index" : "my_index",
        "_type" : "index",
        "_id" : "6022",
        "_score" : 21.000307,
        "_source" : {
          "identifier" : "H2020ICT2015",
          "Text" : """STREAMLINE STREAMLINE will address the competitive advantage needs of European online media businesses EOMB by delivering fast reactive analytics suitable in solving a wide array of problems including addressing customer retention personalised recommendation and more broadly targeted services STREAMLINE will develop crosssectorial analytics drawing on multisource data originating from online media consumption online games telecommunications services and multilingual web content STREAMLINE partners face big and fast data challenges They serve over 100 million users offer services that produce billions of events yielding over 10 TB of data daily and possess over a PB of data at rest Their business usecases are representative of EOMB which cannot be handled efficiently  effectively by stateoftheart technologies as a consequence of system and human latencies System latency issues arise due to the lack of appropriate data streamoriented analytics tools and more importantly the added complexity cost and burden associated with jointly supporting analytics for both data at rest and data in motion Human latency results from the heterogeneity of existing tools and the low level programming languages required for development using an inordinate number of boilerplate codes that are system specific eg Hadoop SolR Esper Storm and databases and a plethora of scripts required to glue systems togetherOur research and innovation actions include addressing the challenges brought on by system and human latencies In this regard STREAMLINE will1	Develop a high level declarative language and userinterface and corresponding automatic optimisation parallelisation and system adaptation technologies that reduce the programming expertise required by data scientists thereby enabling them to more freely focus on domain specific matters2	Overcome the complexity of the socalled lambda architecture by delivering simplified operations that jointly support data at rest and data in motion in a single system and is compatible with the Hadoop ecosystem3	Develop fast reactive machine learning technologies based on distributed parameter servers and fully distributed asynchronous and approximate algorithms for fast results at high input ratesThe impact of developing a European open source tool for analysing data at rest and data in motion in a single system featuring a high level declarative language and a fast reactive machine learning library is much wider than just the recommender ad targeting and customer retention applications that the industrial partners in STREAMLINE will use to demonstrate the business value of our work for the data economy Our open source tools will help Europe in general since they lower the big data analytics skills barrier broaden the reach of data analytics tools and are applicable to diverse market sectors including healthcare manufacturing and transportation Thereby enabling a broad number of European SMEs in other markets to explore and integrate these technologies into their businesses At the same time STREAMLINE will provide a solid foundation for big data leadership in Europe by providing an opensource platform ready to be used by millions of stakeholders in companies households and governmentThe STREAMLINE consortium comprises worldrenowned scientists and innovators in the areas of database systems DFKI distributed systems SICS and machine learning SZTAKI who have won many international awards hold 18 patents collectively and have founded and advised nine startups Complementing the research excellence are four leading European enterprises in the data economy in the areas of global telecommunication services eg Internet IPTV mobile and landline networks PT games and entertainment Rovio media content streaming NMusic and webscale data extraction and business analytics IMR with Petab""",
          "Rcn" : "199862",
          "Acronym" : "STREAMLINE"
        }
      },
      {
        "_index" : "my_index",
        "_type" : "index",
        "_id" : "1159",
        "_score" : 20.78681,
        "_source" : {
          "identifier" : "H2020ICT20141",
          "Text" : "MMT will deliver a language independent commercial online translation service based on a new opensource machine translation distributed architecture The goal of MMT is to deliver a language independent commercial online translation service based on a new opensource machine translation distributed architecture MMT does not require any initial training phase Once fed with training data MMT will be ready to translate MMT defacto will merge translation memory and machine translation technology into one single product Quality of translations will increase as soon as new training data are addedMMT manages context automatically so that it will not require building domain specific systems MMT will provide best translation quality for any topicdomain by storing training segments together with context linking informationMMT enables scalability of data and users so that no more expensive adhoc hardware installations are needed The MMT architecture will support high performance and linear scalability up to thousands of nodes The same software will work to setup a personal translation system or to create a webbased service on a cluster of commodity nodes able to handle terabytes of data and millions of usersMMT will create a data collection infrastructure that accelerates the process of filling the data gap between large IT companies and the MT industry  MMT will leverage the data crawled on the web by Common Crawl TAUS Translateds MyMemory and Matecat data and facilities to set up a processing pipeline that will create unprecedented  amounts of clean parallel and monolingual data to develop machine translation systems",
          "Rcn" : "194327",
          "Acronym" : "MMT"
        }
      },
      {
        "_index" : "my_index",
        "_type" : "index",
        "_id" : "560",
        "_score" : 20.742483,
        "_source" : {
          "identifier" : "ERC2014STG",
          "Text" : "Domain Adaptation for Statistical Machine Translation Rapid translation between European languages is a cornerstone of good governance in the EU and of great academic and commercial interest Statistical approaches to machine translation constitute the stateoftheart The basic knowledge source is a parallel corpus texts and their translations For domains where large parallel corpora are available such as the proceedings of the European Parliament a high level of translation quality is reached However in countless other domains where large parallel corpora are not available such as medical literature or legal decisions translation quality is unacceptably poor Domain adaptation as a problem of statistical machine translation SMT is a relatively new research area and there are no standard solutions The literature contains inconsistent results and heuristics are widely used We will solve the problem of domain adaptation for SMT on a larger scale than has been previously attempted and base our results on standardized corpora and open source translation systemsWe will solve two basic problems The first problem is determining how to benefit from large outofdomain parallel corpora in domainspecific translation systems This is an unsolved problem The second problem is mining and appropriately weighting knowledge available from indomain texts which are not parallel While there is initial promising work on mining weighting is not well studied an omission which we will correct We will scale mining by first using Wikipedia and then mining from the entire webOur work will lead to a breakthrough in translation quality for the vast number of domains with less parallel text available and have a direct impact on SMEs providing translation services The academic impact of our work will be large because solutions to the challenge of domain adaptation apply to all natural language processing systems and in numerous other areas of artificial intelligence research based on machine learning approaches",
          "Rcn" : "193728",
          "Acronym" : "DASMT"
        }
      },
      {
        "_index" : "my_index",
        "_type" : "index",
        "_id" : "1331",
        "_score" : 20.586464,
        "_source" : {
          "identifier" : "ERC2014CoG",
          "Text" : "Chronologicon Hibernicum  A Probabilistic Chronological Framework for Dating Early Irish Language Developments and Literature Early Medieval Irish literature 7th10th centuries is vast in extent and rich in genres but owing to its mostly anonymous transmission for most texts the precise time and circumstances of composition are unknown Unless where texts contain historical references the only clues for a rough chronological positioning of the texts are to be found in their linguistic peculiarities Phonology morphology syntax and the lexicon of the Irish language changed considerably from Early Old Irish 7th c into Middle Irish c 10th12th centuries However only the relative sequence of changes is well understood for most sound changes very few narrow dates have been proposed so far It is the aim of Chronologicon Hibernicum to find a common solution for both problems through the linguistic profiling of externally dated texts esp annalistic writing and sources with a clear historical anchorage and through serialising the emerging linguistic and chronological data progress will be made in assigning dates to the linguistic changes Groundbreakingly this will be done by using statistical methods for the seriation of the data and for estimating dates using Bayesian inferenceThe resultant information will then be used to find new dates for hitherto undated texts On this basis a much tighter chronological framework for the developments of the Early Medieval Irish language will be created In a further step it will be possible to arrive at a better chronological description of medieval Irish literature as a whole which will have repercussions on the study of the history and cultural and intellectual environment of medieval Ireland and on its connections with the wider worldThe data collected and analysed in this project will form the database Chronologicon Hibernicum which will serve as the authoritative guideline and reference point for the linguistic dating of Irish texts In the future the methodology will be transferable to other languages",
          "Rcn" : "194499",
          "Acronym" : "ChronHib"
        }
      },
      {
        "_index" : "my_index",
        "_type" : "index",
        "_id" : "14145",
        "_score" : 20.537209,
        "_source" : {
          "identifier" : "H2020ICT20162",
          "Text" : "X5gon Cross Modal Cross Cultural Cross Lingual Cross Domain and Cross Site Global OER Network The proposal X5gon stands for easily implemented freely available innovative technology elements converging currently scattered Open Educational Resources OER available in various modalities across Europe and the globe X5gon combines content understanding user modelling quality assurance methods and tools to boost a homogenous network of OER sites and provides users teachers learners with a common learning experience X5gon deploys open technologies for recommendation learning analytics and learning personalisation services that works across various OER sites independent of languages modalities scientific domains and sociocultural contexts It develops services OER media convergence including full courses course materials modules textbooks videos tests software related events tools materials techniques used to support access to knowledge Fivefold solutions are offered to OER sites Crossmodal technologies for multimodal content understanding Crosssite technologies to transparently accompany and analyse users across sites Crossdomain technologies for cross domain content analytics Crosslanguage technologies for cross lingual content recommendation Crosscultural technologies for cross cultural learning personalisationX5gon collects and index OER resources track data of users progress and feed an analytics engine driven by stateoftheart machine learning improve recommendations via user understanding and match with knowledge resources of all types The project will create three services X5oerfeed X5analytics and X5recommend and run a series of pilot case studies that enable the measurement of the broader goals of delivering a useful and enjoyable educational experience to learners in different domains at different levels and from different cultures Two exploitation scenarios are planned i free use of services for OER ii commercial exploitation of the multimodal big data realtime analytics pipeline",
          "Rcn" : "211074",
          "Acronym" : "X5gon"
        }
      },
      {
        "_index" : "my_index",
        "_type" : "index",
        "_id" : "15743",
        "_score" : 20.473845,
        "_source" : {
          "identifier" : "H2020SC6REVINEQUAL2017",
          "Text" : "Current European and CrossNational Comparative Research and Research Actions on Migration Migration and the characteristics which constitute its parameters dynamics and complexities comprises one of the most paramount matters in contemporary Europe Under these designated circumstances the necessity of relevant concise and useful knowledge are prerequisites for the design of efficient and constructive policies Although particular databases such as EUROSTAT and OECD offer valuable insights into these migratory dynamics a comprehensive efficient and integrative database which synthesizes categorizes and maps out the vast analytical accounts on migration throughout Europe is nonexistent This project bringing together 16 leading research institutions networks and policy institutes throughout Europe aims to proficiently fulfill this gap crucial for policypurposes through the construction of a central migration hub This hub will be of instrumental value due to its capability to operate as a key grammar in the design of current and future policy Essentially it accumulates and consolidates past present and future migration research through providing an extensive yet succinct overview of migration drivers infrastructures flows and policies allowing for an improved systematic understanding of the factors that constitute the interaction between these analytical categories The accessibility accumulation and integration of research in one hub will be an integral element for improved policy making as it concentrates and visualizes relevant data  thereby facilitating information acquisition in pursuance of policy oriented goals As of such a continuous researchpolicy dialogue is prevalent throughout the construction of the hub an insight which enables its users to visualize and develop migration scenarios entailing a classification system for migration research  Consequently the project aims to shape a strategic research agenda on migration as it will identify gaps overlaps and connections within the available stock of migration research",
          "Rcn" : "212877",
          "Acronym" : "CROSS-MIGRATION"
        }
      },
      {
        "_index" : "my_index",
        "_type" : "index",
        "_id" : "7318",
        "_score" : 19.837486,
        "_source" : {
          "identifier" : "H2020MSCAIF2015",
          "Text" : "Probabilistic Argumentation on the Web The World Wide Web hosts a wide range of argumentative text from resources of multiple disciplines and online debates Also tools such as Debadepedia and Twitter encourage the communication of arguments in social and scientific settings With the exponential growth of the Web and its users a vast amount of argumentative text on the Web remains hidden In order to query the Web for structured arguments included in web pages it is necessary to address both of the following issues 1 the deployment of technologies that enable an automatic extraction of the components of natural language arguments and the representation of their meaning and 2 the deployment of a pragmatic argumentation formalism that takes into account the uncertain and inconsistent nature of data on the Web to reason with structured arguments Stateoftheart research in natural language processing NLP recently engaged in the deployment of technologies for learning the semantic similarity between statements and for the extraction of probabilistic beliefs and logic expressions from natural language text This is a promising direction forward toward the automatic extraction of the components of argumentative text online Additionally research on probabilistic formalisms supporting argumentation reasoning is at the heart of stateoftheart research in knowledge representation and reasoning KRR The goal of the ARGUEWEB project is to develop a scalable probabilistic argumentation system for the retrieval for the  principled management of points of view derived from argumentative text on web pages and  for query answering from such points of view One of the central aspects of this scalable approach is the representation of structured arguments using an ontology language and the development of a formalism which is tolerant to uncertainty and inconsistency",
          "Rcn" : "201509",
          "Acronym" : "ARGUE_WEB"
        }
      }
    ]
  }
}
